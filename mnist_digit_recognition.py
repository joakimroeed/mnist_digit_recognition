# -*- coding: utf-8 -*-
"""MNIST Digit Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FDoWrOcPwbmpNUnmGXbrVFUZbjYtKPLj
"""

# This program classifies the MNIST Handwritten Digits images as a number 0 - 9

# Import dependencies
import numpy as np
import matplotlib.pyplot as plt 
import keras
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical, np_utils
from keras.datasets import mnist
from keras.callbacks import EarlyStopping

# Split MNIST data into training and testing data
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

# Adding a depth dimension to the data, so our MNIST data are single channeled
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)

# Cast the data to float32
X_train = X_train.astype('float32') 
X_test = X_test.astype('float32')

# Normalize the pixel values from [0, 255] 
X_train = (X_train / 255)
X_test = (X_test / 255)

# Convert class vectors to 10-dimensional class matrices
y_train = np_utils.to_categorical(y_train, 10)
y_test = np_utils.to_categorical(y_test, 10)

# Designing the model
model = Sequential() 

# layer 1
model.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (28, 28, 1)))

# Layer 2
model.add(Conv2D(filters = 64, kernel_size = 3, activation= 'relu'))

# Layer 3 
model.add(Conv2D(filters = 128, kernel_size = 3, activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

# Layer 4
model.add(Conv2D(filters = 256, kernel_size = 3, activation = 'relu'))

# Layer 5
model.add(Conv2D(filters = 512, kernel_size = 3, activation = 'relu'))

# Layer 6
model.add(Conv2D(filters = 1024, kernel_size = 3, activation = 'relu'))

# Output 
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(10, activation = 'softmax'))  
          
#print(model.summary())

# Compiling our model
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

early_stop = EarlyStopping(monitor = 'val_loss', patience = 1)

# Fitting (training) the model
hist = model.fit(X_train, y_train, validation_split = 0.1, validation_data = (X_test, y_test), epochs = 10, batch_size = 10)
test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)

score = model.evaluate(X_test, y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

model_json = model.to_json()
with open("model.json", "w") as json_file: # # model.json hold the architecture of the model I just built
  json_file.write(model_json)
model.save_weights("model.h5") # model.h5 is a binary file which holds the weights
print("Saving the model and weight as model.json and model.h5 respectively")

# summarize history for accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# summarize history for loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()